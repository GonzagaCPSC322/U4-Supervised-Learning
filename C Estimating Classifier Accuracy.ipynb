{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Estimating Classifier Accuracy\n",
    "What are our learning objectives for this lesson?\n",
    "* Divide a dataset into training and testing sets using different approaches\n",
    "* Evaluate classifier performance using accuracy\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "Open ClassificationFun/main.py\n",
    "1. Review the PA4 starter code from last class. You can grab mine from Github/U4/ClassificationFun.\n",
    "1. Let's make sure [Sci-kit Learn's `KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) finds the same `k=3` closest neighbors of `[2, 3]` as the starter code computes:\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "...\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\")\n",
    "knn_clf.fit(X_train, y_train)\n",
    "print(knn_clf.kneighbors([test_instance]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Nice job getting PA3 done! On to...\n",
    "    * PA4 is due one week from tomorrow. A few notes:\n",
    "        * All unit tests in `test_myclassifiers.py` are desk calculations. If a unit test should test against a library, this will be described.\n",
    "        * See my docstring note for `MyKNeighborsClassifier`: `Assumes data has been properly normalized before use.`. It is the calling code's responsibility to normalize data before use with `MyKNeighborsClassifier`\n",
    "        * Questions?\n",
    "* Estimating classifier accuracy\n",
    "    * Dividing a dataset into training and test sets\n",
    "    * Lab tasks\n",
    "* IQ4 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today \n",
    "* Announcements\n",
    "    * Let's go over IQ4, nice job!\n",
    "    * The career fair is coming up on 10/24 12-4!\n",
    "    * MA7 and MA8 are posted.\n",
    "    * PA4 is due Friday. Questions?\n",
    "    * PA5 is posted. Please read it before next class and come with questions\n",
    "* Overview of Naive Bayes\n",
    "    * Lab tasks\n",
    "    * MA7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "Building a classifier starts with a learning (training) phase\n",
    "* Based on predefined set of examples (AKA the training set)\n",
    "\n",
    "The classifier is then evaluated for predictive accuracy (% of test instances correctly classified by the classifier)\n",
    "* Based on another set of examples (AKA the testing set)\n",
    "* We use the actual labels of the examples to test the predictions\n",
    "\n",
    "In general, we want to try to avoid overfitting\n",
    "* That is, encoding particular characteristics/anomalies of the training set into the classifier\n",
    "* Similar notion is \"underfitting\" (too simple of a model, e.g., linear instead of polynomial)\n",
    "\n",
    "We are going to discuss different ways to select training and testing sets\n",
    "1. The Holdout method\n",
    "2. Random Subsampling\n",
    "3. $k$-Fold Cross Validation and Variants\n",
    "4. Bootstrap Method\n",
    "\n",
    "### Holdout Method\n",
    "In the holdout method, the dataset is divided into two sets, the training and the testing set. The training set is used to build the model and the testing set is used to evaluate the model (e.g. the model's accuracy).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "\n",
    "Approaches to the holdout method\n",
    "* Randomly divide data set into a training and test set\n",
    "* Partition evenly or, e.g., $\\frac{2}{3}$ to $\\frac{1}{3}$ (2:1) training to test set\n",
    "* This is random selection without replacement\n",
    "\n",
    "Q: Write a function to do a 2:1 partition in Python...\n",
    "```python\n",
    "import random\n",
    "\n",
    "def compute_holdout_partitions(table):\n",
    "    # randomize the table\n",
    "    randomized = table[:] # copy the table\n",
    "    n = len(table)\n",
    "    for i in range(n):\n",
    "        # pick an index to swap\n",
    "        j = random.randrange(0, n) # random int in [0,n) \n",
    "        randomized[i], randomized[j] = randomized[j], randomized[i]\n",
    "    # return train and test sets\n",
    "    split_index = int(2 / 3 * n) # 2/3 of randomized table is train, 1/3 is test\n",
    "    return randomized[0:split_index], randomized[split_index:]\n",
    "```\n",
    "\n",
    "### Random Subsampling Method\n",
    "* Repeat the holdout method $k$ times\n",
    "* Accuracy estimate is the average of the accuracy of each iteration\n",
    "\n",
    "### k-Fold Cross-Validation Method\n",
    "One of the shortcomings of the hold out method is the evaluation of the model depends heavily on which examples are selected for training versus testing. K-fold cross validation is a model evaluation approach that addresses this shortcoming of the holdout method.\n",
    "* Initial dataset partitioned into $k$ subsets (\"folds\") $D_1, D_2,..., D_k$\n",
    "* Each fold is approximately the same size\n",
    "* Training and testing is performed $k$ times:\n",
    "    * In iteration $i$, $D_i$ is used as the test set\n",
    "    * And $D_1 \\cup ... \\cup D_{iâˆ’1} \\cup D_{i+1} \\cup ... \\cup D_k$ used as training set\n",
    "* Note each subset is used exactly once for testing\n",
    "* Accuracy estimate is number of correct classifications over the $k$ iterations, divided by total number of rows (i.e., test instances) in the initial dataset\n",
    "* Alternatively, average accuracy by label\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "### Variants of Cross-Validation\n",
    "Leave-one-out method\n",
    "* Special case of cross-validation where $k$ is the number of instances\n",
    "\n",
    "Stratified Cross-Validation method\n",
    "* Class distribution within folds is approximately the same as in the initial data\n",
    "\n",
    "Q: How might you go about generating stratified folds for cross validation?\n",
    "* One approach:\n",
    "    * Randomize the dataset\n",
    "    * Partition dataset so each subset contains rows with of a specific class\n",
    "        * e.g., if class label is \"yes\" or \"no\"\n",
    "        * Then one partition has all \"yes\" rows\n",
    "        * And the other all \"no\" rows\n",
    "        * Note: this is a group by class label\n",
    "    * Generate folds by:\n",
    "        * Iterating through each partition\n",
    "        * And distributing the partition (roughly) equally to each fold\n",
    "        \n",
    "### Lab Task 1\n",
    "Consider the following dataset:\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. Assume we want to perform k-fold cross validation of our NN classifier for $k$ = 4. Create corresponding folds (partitions) for the dataset.\n",
    "2. Describe how these $k$ folds would be used to perform cross validation. That is, show how the $k$ test runs are performed.\n",
    "3. Repeat steps 1 and 2 with *stratified* k-fold cross validation.\n",
    "\n",
    "    \n",
    "### The Bootstrap Method\n",
    "* Like random subsampling but with replacement\n",
    "* Usually used for small datasets\n",
    "* The basic \".632\" approach:\n",
    "    * Given a dataset with $D$ rows\n",
    "    * Randomly select $D$ rows with replacement (i.e., might select same row)\n",
    "    * This gives a \"bootstrap sample\" (training set) of $D$ rows\n",
    "    * The remaining rows (not selected; AKA out of bag instances) form the test set\n",
    "    * On average, 63.2% of original rows will end up in the training set\n",
    "    * And 36.8% will end up in the test set\n",
    "* Why these percentages?\n",
    "    * Each row has a $1/D$ chance of being selected\n",
    "    * Each row has a (1 - 1/D) chance of not being selected\n",
    "    * We select $D$ times, so probability a row not chosen at all is $(1 - 1/D)^D$\n",
    "    * For large $D$, the probability approaches $e^{-1} = 0.368$ (for $e = 2.718$...)\n",
    "* The sampling procedure is repeated $k$ times\n",
    "    * Each iteration uses the test set for an accuracy estimate\n",
    "    * Since each test can be a different size based on the sampling, the accuracy is the weighted average accuracy over the $k$ bootstrap methods\n",
    "        * See Bramer 7.2.2 for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier Performance\n",
    "Divide data set into a Training Set and a Test Set\n",
    "* \"Build\" classifier on training set\n",
    "* Test performance on the test set (try to predict their labels)\n",
    "* For the test set you know the \"ground truth\"\n",
    "\n",
    "Assume we have 2-valued class labels (e.g., \"yes\" and \"no\")\n",
    "* As an example, the titanic data set (more later)\n",
    "\n",
    "|status |age |gender |survived|\n",
    "|-|-|-|-|\n",
    "|crew |adult |female |yes|\n",
    "|first |adult |male |no|\n",
    "|crew |child |female |no|\n",
    "|second |adult |male |yes|\n",
    "\n",
    "* We want to predict/classify survival (i.e., survived is the class)\n",
    "    * Positive instances: instances of the \"main\" class of interest (e.g., yes label)\n",
    "    * Negative instances: all the other instances\n",
    "* $P$ = the # of positive instances in our test set\n",
    "* $N$ = the # of negative instances in our test set\n",
    "* $TP$ = (True Positives) = # of positive instances we classified as positive\n",
    "* $TN$ = (True Negatives) = # of negative instances we classified as negative\n",
    "    * Combined, these are our \"successful\" predictions\n",
    "* $FP$ (False Positives) = # of negative instances we classified as positive\n",
    "* $FN$ (False Negatives) = # of positive instances we classified as negative\n",
    "    * Combined, these are our \"failed\" predictions\n",
    "\n",
    "A generalized \"confusion matrix\" for (binary) classification:  \n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/binary_confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "Note: Sci-kit Learn's [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) returns a confusion matrix given parallel lists of actual and predicted values.\n",
    "\n",
    "## Confusion Matrix Application: Computing Accuracy\n",
    "Let's see how confusion matrices can help us calculate accuracy and interpret classifier performance. We will start with binary classification, then we will see how accuracy (and other metrics) can be adapted to multi-class classification (e.g. 3 or more class labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy: The proportion of instances that are correctly classified\n",
    "$$Accuracy = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + FP + TN + FN}$$\n",
    "* Sometimes called \"recognition rate\"\n",
    "* Referred to as \"predictive accuracy\" in the textbook\n",
    "* Warning: can be skewed if unbalanced distribution of class labels\n",
    "    * e.g., lots of negative cases that are easily detected (e.g. 99% accuracy when 99% of the dataset is the negative class)\n",
    "    * shadows performance on positive cases\n",
    "* Note: Sci-kit Learn's (`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) returns prediction accuracy given parallel lists of actual and predicted values.\n",
    "    \n",
    "### Lab Task 2\n",
    "What is the accuracy for the following binary classification confusion matrix?\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/accuracy_exercise.png\" width=\"300\">\n",
    "\n",
    "[//]: # ($$Accuracy = \\frac{TP + TN}{P + N} = \\frac{18 + 8}{40} = 65\\%$$)\n",
    "    \n",
    "### Accuracy for Multi-class Classification\n",
    "Q: How do we adopt/apply accuracy to MPG data set (lots of classes)?\n",
    "* This is called \"multi-class classification\" (vs \"binary\" classification)\n",
    "\n",
    "Multi-Class Accuracy Example: Assume labels $L = \\{a, b, c\\}$ and $R = \\#$ of total instances\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_confusion_matrix.png\" width=\"400\">\n",
    "\n",
    "* One approach (the micro approach):\n",
    "    * Number of correctly classified divided by total number of instances $Accuracy = \\frac{TP + TN}{P+N} = \\frac{R_a^a + R_b^b + R_c^c}{R}$\n",
    "    * Easily skewed by certain classes\n",
    "* Another approach (the macro approach):\n",
    "    * Average accuracy per class label\n",
    "    * Basically one binary confusion matrix per label (then average of these)\n",
    "    * If $L$ is the number of labels: $$\\frac{\\sum_{i=1}^{L}\\frac{TP_i+TN_i}{P_i+N_i}}{L}$$\n",
    "    * Have to be careful of empty classes in the test set (don't include in $L$)\n",
    "    \n",
    "To compute the accuracy of label \"a\": $$Accuracy_a = \\frac{TP_a + TN_a}{P_a+N_a}\\\\= \\frac{R_a^a + (R_b^b + R_b^c +R_c^b+R_c^c)}{R_a + (R_b + R_c)}\\\\=\\frac{R - (FN_a +FP_a)}{R}\\\\=\\frac{R - (R_a^b+R_a^c+R_b^a+R_c^a)}{R}$$\n",
    "\n",
    "We could do this for each label, then average the results\n",
    "\n",
    "### Lab Task 3\n",
    "What is the accuracy for the following multi-class classification confusion matrix?\n",
    "\n",
    "Coffee acidity labels: dry, sharp, moderate, or dull\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_class_accuracy_exercise.png\" width=\"400\">\n",
    "\n",
    "1. 1st Approach (percent correctly classified; AKA micro approach):\n",
    "1. 2nd Approach (average accuracy per label; AKA macro approach). First let's do the label dry:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_class_exercise_matrix.png\" width=\"400\">\n",
    "\n",
    "Then finish the approach for the remaining labels:  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
