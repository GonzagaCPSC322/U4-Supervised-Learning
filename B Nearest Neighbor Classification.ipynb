{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Nearest Neighbor Classification\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about the kNN classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. (first, together) Create a new folder called ClassificationFun and create a `main.py`\n",
    "    * Let's take some quick notes on programming interfaces for ML algorithms\n",
    "1. (later) Add `mysimplelinearregressor.py` from https://github.com/GonzagaCPSC322/U4-Supervised-Learning/tree/master/ClassificationFun\n",
    "    * Read through the module\n",
    "1. (later) In main.py. create a `MySimpleLinearRegressor` object and call `fit()` using our \"y = 2x + some noise\" data as the training data. Then try calling `predict()` for an unseen instance e.g. `[150]`\n",
    "    Note: `X_train` and `X_test` need to be 2D... `X` is a feature \"matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today \n",
    "* Announcements\n",
    "    * No MA this week ðŸŽ‰\n",
    "    * PA3 is due Tuesday. Questions?\n",
    "    * Grad school/career prep advice talk tonight, 6pm Bollier 120\n",
    "        * There will be pizza and IQ4 bonus point question content ðŸ¤“\n",
    "    * New club: Zags AI Lab (ZAIL)\n",
    "        * Hosting a coding challenge that opens Friday 10/4: https://zail-challenge-2024.pangeon.com/\n",
    "* Unit testing of `MySimpleLinearRegressor`\n",
    "* kNN overview and algorithm trace\n",
    "* IQ3 last ~15 mins of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s) \n",
    "1. Accept, clone, and open PA4\n",
    "1. Read through the `MySimpleLinearRegressionClassifier` docstrings in `myclassifiers.py`\n",
    "    * What is the purpose of `discretizer`?\n",
    "    * Functions in Python are objects, therefore a function name is a reference. See code snippet below\n",
    "1. We are going to practice TDD (test-driven development... writing unit tests before writing the units themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'> <class 'function'> True\n",
      "10\n",
      "orig list: [3, 4, 5]\n",
      "map double to each elem: [6, 8, 10]\n",
      "filter each elem by odd: [3, 5]\n",
      "reduce elems to total value: 12\n"
     ]
    }
   ],
   "source": [
    "def double(x):\n",
    "    return 2 * x\n",
    "\n",
    "myfunc = double\n",
    "print(type(double), type(myfunc), double == myfunc)\n",
    "print(myfunc(5))\n",
    "\n",
    "# often used with map, filter, reduce\n",
    "nums = [3, 4, 5]\n",
    "print(\"orig list:\", nums)\n",
    "# map a function to each elem in a list\n",
    "print(\"map double to each elem:\", list(map(myfunc, nums)))\n",
    "\n",
    "# filter elems in a list by some criteria\n",
    "def odd(x):\n",
    "    return x % 2 == 1\n",
    "print(\"filter each elem by odd:\", list(filter(odd, nums)))\n",
    "\n",
    "# reduce elems in a list to a single value\n",
    "from functools import reduce\n",
    "def total(total_so_far, x):\n",
    "    return total_so_far + x\n",
    "print(\"reduce elems to total value:\", reduce(total, nums, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    "* Announcements\n",
    "    * Nice job on IQ3, let's go over it.\n",
    "    * IQ4 on Thursday on Jupyter Notebook/Markdown/Latex\n",
    "    * PA3 is due tonight. Questions?\n",
    "    * Work on PA4\n",
    "    * RQ5 is posted and due **next Thursday 10/20** (so you can check your work after some examples of Naive Bayes we will do next week)\n",
    "* PA4 starter code and hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Starter Code and Hints\n",
    "1. kNN example trace (on iPad)\n",
    "1. In ClassificationFun/main.py, write a unit test for a function called `compute_euclidean_distance()` that accepts two feature vectors and returns the Euclidean distance between them: $dist(v1, v2) = \\sqrt{\\sum_{i=1}^{n}(v1_i - v2_i)^{2}}$\n",
    "    * Make up some values to call your function with\n",
    "    * Test your function's output against SciPy's `euclidean()` function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html\n",
    "    1. Write the `compute_euclidean_distance(v1, v2)` function and test it with `pytest main.py`\n",
    "1. In ClassificationFun/main.py, consider the following labeled dataset, represented as the lists below. **Assume the two attributes have been normalized by scaling to a value between 0 and 10 (no additional normalization is necessary)**\n",
    "    1. Copy and paste the following lists into your main.py\n",
    "    1. Compute the distance between each train instance and the unseen instance: `[2, 3]`\n",
    "        * Print out the distances and inspect them...\n",
    "        * What are the k=3 closest neighbors of the following instance and how would you classify it?\n",
    "            * How can you programmatically answer this question?\n",
    "            * Hint: need a way to sort training instances by distance\n",
    "\n",
    "```python\n",
    "header = [\"att1\", \"att2\"]\n",
    "X_train = [\n",
    "    [3, 2],\n",
    "    [6, 6],\n",
    "    [4, 1],\n",
    "    [4, 4],\n",
    "    [1, 2],\n",
    "    [2, 0],\n",
    "    [0, 3],\n",
    "    [1, 6]\n",
    "]\n",
    "y_train = [\"no\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"yes\"] # parallel to X_train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification\n",
    "Nearest neighbor classification is typically used when attributes are continuous\n",
    "* Can be modified for categorical data ...\n",
    "\n",
    "Basic approach:\n",
    "* Given an instance $i$ with $n - 1$ attributes (where the $n^{th}$ is class label)\n",
    "* Find the \"closest\" instance $j$ to $i$ on the $n - 1$ attributes\n",
    "* Use $j$'s class as the prediction for $i$\n",
    "\n",
    "Example from book:\n",
    "Given the data set\n",
    "\n",
    "|a |b |c |d |e |f |Class|\n",
    "|-|-|-|-|-|-|-|\n",
    "|yes |no |no |6.4 |8.3 |low |negative|\n",
    "|yes |yes |yes |18.2 |4.7 |high |positive|\n",
    "\n",
    "What should this instance's classification be?\n",
    "\n",
    "|yes |no |no |6.6 |8.0 |low |???|\n",
    "|-|-|-|-|-|-|-|\n",
    "\n",
    "Usually it isn't this easy!\n",
    "\n",
    "## k Nearest Neighbors\n",
    "Find the k nearest neighbors ...\n",
    "* Usually find the k closest neighbors (instead of just closest)\n",
    "* Then pick classification from among the top k\n",
    "\n",
    "What are good values for the number of neighbors k?\n",
    "* Often done experimentally (with a test set)\n",
    "* Start with k = 1, determine \"error rate\" (more later)\n",
    "* Repeat incrementing k\n",
    "* Pick k with smallest (minimum) error rate\n",
    "     * Often, larger the data (training) set, the larger the k\n",
    "\n",
    "Lets say we found the k nearest neighbors for an instance ...\n",
    "\n",
    "Q: What are ways we could pick the class?\n",
    "* Most frequent occurring class\n",
    "* Weighted \"average\" (based on the relative closest of the k)\n",
    "\n",
    "Note: can use k-NN for regression if, e.g., return the mean of the label values\n",
    "\n",
    "## Distance Functions\n",
    "k-NN works by calculating distances between instances\n",
    "* Many possible ways to do this ... generalized through \"distance measures\"\n",
    "* For two points $x$ and $y$, the distance between them is given by $dist(x,y)$\n",
    "\n",
    "Properties of distance measures (metrics)\n",
    "1. $\\forall x$, $dist(x,x) = 0$\n",
    "    * The distance of any point x from itself is zero\n",
    "2. $\\forall xy$, $dist(x,y) = dist(y,x)$\n",
    "    * Symmetry\n",
    "3. $\\forall xyz$, $dist(x,y) \\leq dist(x,z) + dist(z,y)$\n",
    "    * Triangle equality\n",
    "    * \"Shortest distance between any two points is a straight line\"\n",
    "\n",
    "Euclidean Distance is most often used: Given an instance, treat it as a \"vector\" in n space\n",
    "* Use Pythagoras' Theorem to find distance between them\n",
    "\n",
    "For example:\n",
    "* Given two points (i.e., rows) with $n = 2$: $(x_1, y_1)$ and $(x_2, y_2)$\n",
    "* The length (i.e., distance) of the straight line joining the points is:\n",
    "\n",
    "$$\\sqrt{(x_1 - x_2)^{2} + (y_1 - y_2)^{2}}$$\n",
    "\n",
    "* Euclidean $n$-space\n",
    "    * For rows A = $(a_1, a_2,..., a_n)$ and $B = (b_1, b_2,..., b_n)$ with $n$ attributes\n",
    "$$\\sqrt{(a_1 - b_1)^{2} + (a_2 - b_2)^{2} +...+ (a_n - b_n)^{2}}$$\n",
    "        * Which is:\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^{2}}$$\n",
    "\n",
    "Other examples are described in the book (e.g., Manhattan \"city block\" distance)\n",
    "\n",
    "Q: Do you see any possible issues with Euclidean distance?\n",
    "* Larger values tend to dominate smaller ones\n",
    "* Can degrade into a few attributes driving the distances\n",
    "    * e.g., [Mileage=18,457, Doors=2, Age=12]\n",
    "    \n",
    "One solution is to scale all values between 0 and 1 (\"min-max\" normalization)\n",
    "* Use the formula: `(x - min(xs)) / ((max(xs) - min(xs)) * 1.0)`\n",
    "\n",
    "Q: How can we deal with categorical values?\n",
    "* $dist(v_1, v_2) = 0$ if values $v_1 = v_2$\n",
    "    * Same values\n",
    "* For nominal values, $dist(v_1, v_2) = 1$ if $v_1 \\neq v_2$\n",
    "    * Different values\n",
    "* For ordinal values, assign to 1 or use the \"distance\"\n",
    "\n",
    "Q: What do we do about missing values?\n",
    "* Don't have missing values\n",
    "    * Clean the data first\n",
    "* Be conservative\n",
    "    * Assuming normalized\n",
    "    * If only one value missing:\n",
    "        * Assume the maximum possible distance\n",
    "        * If nominal use the maximum distance (i.e., 1)\n",
    "        * If ordinal use either 1 or furthest distance from known value\n",
    "    * otherwise, if both values missing, use the maximum distance (e.g., 1)\n",
    "\n",
    "Distance-based metrics imply equal weighting of attributes\n",
    "* Sometimes can perform better with attribute weights (i.e., some attributes worth more)\n",
    "* \"Feature reduction\" (not using certain attributes) can also help with redundant or \"noisy\" attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic k-NN Algorithm\n",
    "```\n",
    "Input: X_train, y_train, X_test, k\n",
    "row_distances = []\n",
    "y_predicted = []\n",
    "for test_instance in X_test:\n",
    "    for train_instance in X_train:\n",
    "        d = distance(train_instance, test_instance)\n",
    "        row_distances.append([d, row])\n",
    "    top_k_instances = get_top_k_instances(row_distances, k) # TODO: need to write this function\n",
    "    prediction = select_class_label(top_k_instances)\n",
    "    y_predicted.append(prediction)\n",
    "```\n",
    "\n",
    "## kNN Example 1\n",
    "Example adapted from [this kNN example](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n",
    "\n",
    "Suppose we have the following dataset that has two attributes (acid durability and strength) and a class attribute (whether a special paper tissue is good or not):\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|7|7|Bad|\n",
    "|7|4|Bad|\n",
    "|3|4|Good|\n",
    "|1|4| Good|\n",
    "\n",
    "Now the factory produces a new paper tissue with acid durability = 3 seconds and strength = 7 kg/square meter. Can we predict what the classification of this new tissue is? \n",
    "\n",
    "Use kNN with $k$ = 3. Steps:\n",
    "1. Normalize\n",
    "1. Compute distance of each training instance to the test instance\n",
    "1. Determine the majority classification of the $k$ closest instances... this is your prediction for the test instance\n",
    "\n",
    "    \n",
    "### Some Notes\n",
    "Q: What happens if there are ties in the top-k distances (get_top_k)? E.g., which are top 3 in: [[.28,$r_1$],[.33,$r_2$],[.33,$r_3$],[.33,$r_4$],[.37,$r_5$]]?\n",
    "* Different options ... e.g.:\n",
    "    * Randomly select from ties\n",
    "    * Do top-k distances (instead of instances)\n",
    "    * Ignore ties (in case above, just use $r_1$ and $r_2$)\n",
    "\n",
    "Nearest doesn't imply near\n",
    "* top-k instances might not be that close to the instance being classified\n",
    "* Especially true as the number of attributes (\"dimensions\") increases\n",
    "    * An example of the \"curse of dimensionality\"\n",
    "* Again, have to use common sense and an understanding of the dataset\n",
    "\n",
    "## kNN Example 2\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have continuous values. Assume the data has been normalized by scaling to a value between 0 and 10.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "Assume we have the following instance to classify using $k$-NN for $k$ = 3 with \"majority voting\" to determine the class label from the k closest neighbors. \n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|2|3|?|\n",
    "\n",
    "1. What are the three closest neighbors of the following instance?\n",
    "1. How would you classify it?\n",
    "\n",
    "### Efficiency issues\n",
    "Q: Is k-NN efficient? Can you find any efficiency issues?\n",
    "* Given a training set with $D$ instances and $k = 1$\n",
    "* $O(D)$ comparisons needed to classify a given instance\n",
    "\n",
    "Q: Can you think of any ways to improve the efficiency?\n",
    "1. Use search trees\n",
    "    * Presort and arrange instances into a search tree\n",
    "    * Can reduce comparisons to $O(log D)$\n",
    "2. Check each training instance in parallel\n",
    "    * Gives $O(1)$ comparisons\n",
    "3. Editing/Pruning\n",
    "    * Filter or remove training tuples that prove useless\n",
    "    * Reduces size of $D$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "03634b338fa45d156389a4bccc161271443b46ab00a525a3df16b6f48f1d4dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
