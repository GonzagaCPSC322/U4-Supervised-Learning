{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Measuring Classifier Performance\n",
    "What are our learning objectives for this lesson?\n",
    "* Measure and evaluate classifier performance using different metrics\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Accuracy: Additional Performance Evaluation Metrics\n",
    "In Bramer Chapter 12, there is a nice table summarizing commonly used performance metrics for a classifier:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/bramer_perf_measures.png\" width=\"600\">\n",
    "\n",
    "Beyond accuracy, let's look at a few more of these in detail. \n",
    "\n",
    "### Error Rate\n",
    "Error Rate: 1 - accuracy\n",
    "$$ErrorRate = \\frac{FP + FN}{P + N}$$\n",
    "* Has same issues as accuracy (unbalanced labels)\n",
    "* For multi-class classification, can take the average error rate per class\n",
    "\n",
    "### Precision\n",
    "Precision (AKA positive predictive value): Proportion of instances classified as positive that are really positive\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "* A measure of \"exactness\"\n",
    "* When a classifier predicts positive, it is correct $precision$ percent of the time\n",
    "* A classifier with no false positives has a precision of 1\n",
    "\n",
    "### Recall\n",
    "Recall (AKA true positive rate (TPR) AKA sensitivity): The proportion of positive instances that are correctly classified as positive (e.g. labeled correctly)\n",
    "$$Recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}$$\n",
    "* A measure of \"completeness\"\n",
    "* A classifier correctly classifies $recall$ percent of all positive cases\n",
    "* A classifier with no false negatives has a precision of 1\n",
    "* Used with the false positive rate to create receiver operator graphs and curves (ROC)\n",
    "\n",
    "Note: There is a trade-off between precision and recall. For a balanced class dataset, a model that predicts mostly positive examples will have a high recall and a low precision.\n",
    "\n",
    "Q: How can we get a high recall score?\n",
    "* Label everything as positive\n",
    "* Note that precision helps keep us honest\n",
    "\n",
    "Q: What about for precision?\n",
    "* Be conservative with our positive labels\n",
    "\n",
    "### F1 Score \n",
    "F1-Score (AKA F-Measure): combines precision and recall via the harmonic mean of precision and recall:\n",
    "$$F = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$$\n",
    "* Summarizes a classifier in a single number (however, it is best practice to still investigate precision and recall, as well as other evaluation metrics)\n",
    "* Alternatively, we can weight precision:\n",
    "$$F_\\beta = \\frac{(1+\\beta^2) \\times Precision \\times Recall}{\\beta^2 \\times Precision + Recall}$$\n",
    "* Helps deal with class imbalance problem\n",
    "\n",
    "Note: Sci-kit Learn's [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) returns multi-class precision, recall, f1-score, and support given parallel lists of actual and predicted values.\n",
    "\n",
    "### Lab Task 1\n",
    "What is the precision, recall, and F-measure for the win-lose (binary) example?\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/accuracy_exercise.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        lose       0.80      0.40      0.53        20\n",
      "         win       0.60      0.90      0.72        20\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.70      0.65      0.63        40\n",
      "weighted avg       0.70      0.65      0.63        40\n",
      "\n",
      "{'lose': {'precision': 0.8, 'recall': 0.4, 'f1-score': 0.5333333333333333, 'support': 20}, 'win': {'precision': 0.6, 'recall': 0.9, 'f1-score': 0.7200000000000001, 'support': 20}, 'accuracy': 0.65, 'macro avg': {'precision': 0.7, 'recall': 0.65, 'f1-score': 0.6266666666666667, 'support': 40}, 'weighted avg': {'precision': 0.7, 'recall': 0.65, 'f1-score': 0.6266666666666667, 'support': 40}}\n"
     ]
    }
   ],
   "source": [
    "# check trace desk calculation with sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# build parallel lists to represent win lose predictions\n",
    "y_true = [\"win\"] * 20 + [\"lose\"] * 20\n",
    "y_pred = [\"win\"] * 18 + [\"lose\"] * 2\n",
    "y_pred += [\"win\"] * 12 + [\"lose\"] * 8\n",
    "\n",
    "# note that \"support\" is P, is the number of instances in the test set with the positive label\n",
    "# classiciation_report() reports metrics for \"win\" as the positive class\n",
    "# and for \"lose\" as the positive class \n",
    "# nice text/table form\n",
    "print(classification_report(y_true, y_pred))\n",
    "# returned dictionary form\n",
    "report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "print(report_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F-Measure for Multi-class Classification\n",
    "\"Micro\" average $\\mu$\n",
    "* Averaging the total true positives, false negatives and false positives\n",
    "    * E.g. compute TP and FP (or FN) over all the labels to compute precision (or recall))\n",
    "* Micro-averaging favors bigger classes\n",
    "\n",
    "$$Precision_\\mu = \\frac{\\sum_{i=1}^{L} TP_i}{\\sum_{i=1}^{L} (TP_i + FP_i)}$$\n",
    "\n",
    "$$Recall_\\mu = \\frac{\\sum_{i=1}^{L} TP_i}{\\sum_{i=1}^{L}(TP_i + FN_i)}$$\n",
    "\n",
    "$$F_\\mu = \\frac{2 \\times Precision_\\mu \\times Recall_\\mu}{Precision_\\mu + Recall_\\mu}$$\n",
    "\n",
    "\"Macro\" averaging $M$\n",
    "* Averaging the unweighted mean per label\n",
    "    * E.g. compute each label's precision (or recall) and average over number of labels\n",
    "* Macro-averaging treats all classes equally\n",
    "$$Precision_M = \\frac{\\sum_{i=1}^{L}\\frac{TP_i}{TP_i + FP_i}}{L}$$\n",
    "\n",
    "$$Recall_M = \\frac{\\sum_{i=1}^{L}\\frac{TP_i}{TP_i + FN_i}}{L}$$\n",
    "\n",
    "$$F_M = \\frac{\\sum_{i=1}^{L} \\frac{2 * Precision_{Mi} * Recall_{Mi}}{Precision_{Mi} + Recall_{Mi}}}{L}$$\n",
    "\n",
    "\"Weighted\" macro averaging $W$\n",
    "* Averaging the support-weighted mean per label\n",
    "    * E.g. like macro average, but compute each label's precision (or recall) then weight it by its count $P$ (AKA support) and average over the total number of instances\n",
    "$$Precision_W = \\frac{\\sum_{i=1}^{L}P_i \\times \\frac{TP_i}{TP_i + FP_i}}{P + N}$$\n",
    "\n",
    "$$Recall_W = \\frac{\\sum_{i=1}^{L}P_i \\times \\frac{TP_i}{TP_i + FN_i}}{P + N}$$\n",
    "\n",
    "$$F_W = \\frac{\\sum_{i=1}^{L} P_i \\times \\frac{2 * Precision_{Wi} * Recall_{Wi}}{Precision_{Wi} + Recall_{Wi}}}{P + N}$$\n",
    "\n",
    "### Lab Task 2\n",
    "What is the precision, recall, and F-measure for the coffee acidity (multi-class) example?\n",
    "1. Using the \"Micro\" average approach\n",
    "1. Using the \"Macro\" average approach\n",
    "1. Using the \"Weighted\" macro average approach\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC322/U4-Supervised-Learning/master/figures/multi_class_accuracy_exercise.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dry      0.800     0.800     0.800        25\n",
      "        dull      0.480     0.400     0.436        30\n",
      "    moderate      0.720     0.600     0.655        30\n",
      "       sharp      0.500     0.750     0.600        20\n",
      "\n",
      "    accuracy                          0.619       105\n",
      "   macro avg      0.625     0.638     0.623       105\n",
      "weighted avg      0.629     0.619     0.616       105\n",
      "\n",
      "{'dry': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 25}, 'dull': {'precision': 0.48, 'recall': 0.4, 'f1-score': 0.4363636363636364, 'support': 30}, 'moderate': {'precision': 0.72, 'recall': 0.6, 'f1-score': 0.6545454545454547, 'support': 30}, 'sharp': {'precision': 0.5, 'recall': 0.75, 'f1-score': 0.6, 'support': 20}, 'accuracy': 0.6190476190476191, 'macro avg': {'precision': 0.625, 'recall': 0.6375000000000001, 'f1-score': 0.6227272727272728, 'support': 105}, 'weighted avg': {'precision': 0.6285714285714286, 'recall': 0.6190476190476191, 'f1-score': 0.6164502164502165, 'support': 105}}\n"
     ]
    }
   ],
   "source": [
    "# check trace desk calculation with sklearn\n",
    "# build parallel lists to represent coffee acidity predictions\n",
    "y_true = [\"dry\"] * 25 + [\"sharp\"] * 20 + [\"moderate\"] * 30 + [\"dull\"] * 30\n",
    "y_pred = [\"dry\"] * 20 + [\"sharp\"] * 2 + [\"moderate\"] * 2 + [\"dull\"] * 1\n",
    "y_pred += [\"dry\"] * 0 + [\"sharp\"] * 15 + [\"moderate\"] * 1 + [\"dull\"] * 4\n",
    "y_pred += [\"dry\"] * 1 + [\"sharp\"] * 3 + [\"moderate\"] * 18 + [\"dull\"] * 8\n",
    "y_pred += [\"dry\"] * 4 + [\"sharp\"] * 10 + [\"moderate\"] * 4 + [\"dull\"] * 12\n",
    "\n",
    "# nice text/table form\n",
    "print(classification_report(y_true, y_pred, digits=3))\n",
    "# returned dictionary form\n",
    "report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "print(report_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate \n",
    "False Positive Rate (FPR): The proportion of negative instances that are erroneously classified as positive\n",
    "$$False Positive Rate = \\frac{FP}{N} = \\frac{FP}{TN + FP}$$\n",
    "* Used with the true positive rate to create receiver operator graphs and curves (ROC)\n",
    "\n",
    "### False Negative Rate \n",
    "False Negative Rate (FNR): The proportion of positive instances that are erroneously classified as negative = 1 − True Positive Rate\n",
    "$$False Negative Rate = \\frac{FN}{P} = \\frac{FN}{TP + FN}$$\n",
    "\n",
    "### True Negative Rate \n",
    "True Negative Rate (TNR AKA specificity): The proportion of negative instances that are correctly classified as negative\n",
    "$$False Negative Rate = \\frac{TN}{N} = \\frac{TN}{TN + FP}$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
